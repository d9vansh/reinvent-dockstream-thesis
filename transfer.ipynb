{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d05b965-8aac-444a-86c4-2416dd6a2418",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f157ed4-373e-415c-9f3e-25e2b95a9ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import toml\n",
    "project_dir = os.path.expanduser(\"/Users/devanshjain/laboratoire_d_intelligence_artificielle_en_chimie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bd61919-6063-4c3f-8168-fe82d39eb306",
   "metadata": {},
   "outputs": [],
   "source": [
    "dockstream_path = os.path.expanduser(\"/Users/devanshjain/DockStream\")\n",
    "dockstream_env = os.path.expanduser(\"/Users/devanshjain/miniconda3/envs/DockStream\")\n",
    "\n",
    "apo_protein_filename = \"/Users/devanshjain/7xn1_apo.pdb\"  # Change this to your apo protein file name\n",
    "reference_ligand_filename = \"/Users/devanshjain/7xn1_tacrine.pdb\"  # Change this to your reference ligand file name\n",
    "\n",
    "output_prefix = \"p7xn1\"\n",
    "project_dir = os.path.expanduser(\"/Users/devanshjain/laboratoire_d_intelligence_artificielle_en_chimie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "725c08d0-f8dc-47b4-be7f-054386e921c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_preparator = os.path.join(dockstream_path, \"target_preparator.py\")\n",
    "docker = os.path.join(dockstream_path, \"docker.py\")\n",
    "\n",
    "input_data_dir = os.path.join(project_dir, \"input_data\")\n",
    "output_data_dir = os.path.join(project_dir, \"output_data\")\n",
    "logs_dir = os.path.join(project_dir, \"logs\")\n",
    "config_dir = os.path.join(project_dir, \"configs\")\n",
    "lig_docked_dir = os.path.join(output_data_dir, \"ligands_docked\")\n",
    "scores_dir = os.path.join(output_data_dir, \"docking_scores\")\n",
    "\n",
    "# Create necessary directories\n",
    "for directory in [input_data_dir, output_data_dir, logs_dir, config_dir, lig_docked_dir, scores_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Update file paths\n",
    "apo_protein_path = os.path.join(input_data_dir, apo_protein_filename)\n",
    "reference_ligand_path = os.path.join(input_data_dir, reference_ligand_filename)\n",
    "\n",
    "target_prep_path = os.path.join(config_dir, f\"{output_prefix}_target_prep.json\")\n",
    "fixed_pdb_path = os.path.join(input_data_dir, f\"{output_prefix}_fixed_target.pdb\")\n",
    "receptor_path = os.path.join(input_data_dir, f\"{output_prefix}_receptor.pdbqt\")\n",
    "log_file_target_prep = os.path.join(logs_dir, f\"{output_prefix}_target_prep.log\")\n",
    "log_file_docking = os.path.join(logs_dir, f\"{output_prefix}_docking.log\")\n",
    "log_file_reinvent = os.path.join(logs_dir, f\"{output_prefix}_reinvent.log\")\n",
    "\n",
    "docking_path = os.path.join(config_dir, f\"{output_prefix}_docking.json\")\n",
    "ligands_docked_path = os.path.join(lig_docked_dir, f\"{output_prefix}_ligands_docked.sdf\")\n",
    "ligands_scores_path = os.path.join(scores_dir, f\"{output_prefix}_scores.csv\")\n",
    "ligands_conformer_path = os.path.join(lig_docked_dir, f\"{output_prefix}pydantic.sdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7509fe-a6d7-4084-a69d-6511a1ca7dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f503b685-59cc-4f66-a6a9-315d84cab629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a37cf52c-6e2a-4c73-9a7f-57c4e60f64b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1242b8f7-2f67-47a7-91a9-0104fb7e75f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_toml = f\"\"\"\n",
    "\n",
    "run_type = \"sampling\"\n",
    "device = \"cpu\"  # set torch device e.g. \"cpu\"\n",
    "json_out_config = \"_sampling.json\"  # write this TOML to JSON\n",
    "\n",
    "[parameters]\n",
    "\n",
    "## Reinvent: de novo sampling\n",
    "#model_file = \"/Users/devanshjain/REINVENT4/priors/reinvent.prior\"\n",
    "#transfer_model_file\n",
    "model_file = \"/Users/devanshjain/laboratoire_d_intelligence_artificielle_en_chimie/transfer/TL_reinvent.model\"\n",
    "\n",
    "\n",
    "output_file = '/Users/devanshjain/laboratoire_d_intelligence_artificielle_en_chimie/transfer/transfer_sampling.csv'  # sampled SMILES and NLL in CSV format\n",
    "\n",
    "num_smiles = 108  # number of SMILES to be sampled, 1 per input SMILES\n",
    "unique_molecules = true  # if true remove all duplicatesd canonicalize smiles\n",
    "randomize_smiles = true # if true shuffle atoms in SMILES randomly\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b0f1ed1-a9aa-46b3-ba5f-75f57380f59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_path = os.path.join(project_dir, \"sampling_config.toml\")\n",
    "\n",
    "# Parse the TOML string\n",
    "sampling_dict = toml.loads(sampling_toml)\n",
    "\n",
    "# Write the TOML content to a file\n",
    "with open(sampling_path, 'w') as f:\n",
    "    toml.dump(sampling_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "169abe95-ef78-4e62-ac85-37657d558723",
   "metadata": {},
   "outputs": [],
   "source": [
    "!reinvent -l {log_file_reinvent} {sampling_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc224cf-4128-4ce9-b2cc-3dd00581e636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b626057a-155a-438c-98a3-e9e5b2490322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab4fda19-6096-49e6-949f-32c9df4f3bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TransferLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17ad4876-3ca7-40e4-a77e-79d9e5ed65da",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_toml = f\"\"\"\n",
    "\n",
    "run_type = \"transfer_learning\"\n",
    "device = \"cpu\"  # set torch device e.g. \"cpu\"\n",
    "tb_logdir = \"tb_TL\"  # name of the TensorBoard logging directory\n",
    "json_out_config = \"json_transfer_learning.json\"  # write this TOML to JSON\n",
    "\n",
    "[parameters]\n",
    "\n",
    "num_epochs = 3  # number of steps to run\n",
    "save_every_n_epochs = 3  # save checkpoint model file very N steps\n",
    "batch_size = 50\n",
    "num_refs = 100  # number of reference molecules randomly chosen for similarity\n",
    "                # set this to zero for large datasets (>200 molecules)!\n",
    "sample_batch_size = 100  # number of sampled molecules to compute sample loss\n",
    "\n",
    "\n",
    "## Reinvent\n",
    "input_model_file = \"/Users/devanshjain/REINVENT4/priors/reinvent.prior\"\n",
    "smiles_file = \"/Users/devanshjain/smiles.smi\"  # read 1st column\n",
    "output_model_file = \"/Users/devanshjain/laboratoire_d_intelligence_artificielle_en_chimie/transfer/TL_reinvent.model\"\n",
    "validation_smiles_file = \"/Users/devanshjain/smiles.smi\"\n",
    "\n",
    "# Define the type of similarity and its parameters\n",
    "#pairs.type = \"tanimoto\"\n",
    "#pairs.upper_threshold = 1.0\n",
    "#pairs.lower_threshold = 0.7\n",
    "#pairs.min_cardinality = 1\n",
    "#pairs.max_cardinality = 199\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d8704e7-0d98-4969-83f2-44409ea53f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_path = os.path.join(project_dir, \"transfer_config.toml\")\n",
    "\n",
    "# Parse the TOML string\n",
    "transfer_dict = toml.loads(transfer_toml)\n",
    "\n",
    "# Write the TOML content to a file\n",
    "with open(transfer_path, 'w') as f:\n",
    "    toml.dump(transfer_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "badecf2b-0bb6-442e-8654-6a3f06de0080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|\u001b[32m                                                                         \u001b[0m|00:00\u001b[0m\n",
      "Epoch 3: |\u001b[32m################################################################\u001b[0m|00:01\u001b[0m\n",
      "0it [00:01, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "!reinvent -l {log_file_reinvent} {transfer_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3e0a81-b32f-4488-ba10-1b88a905ee98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af04517a-cc4a-4366-ad4d-f6f7ab8593b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.19.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=tb_TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f8992c-d2a6-4ac2-aa70-ba1641ac0f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759e36da-5f5f-4a2d-85ec-612776236ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37b0bc4e-36ae-43ec-ad49-6a2cc25ece7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#staged-curriculum-reinforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bc5ccb7-604a-46e0-9125-e4af87f663e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "staged_toml = f\"\"\"\n",
    "# REINVENT4 TOML input example for reinforcement/curriculum learning\n",
    "\n",
    "run_type = \"staged_learning\"\n",
    "device = \"cpu\"  \n",
    "tb_logdir = \"tb_RL\"  # Relative path to the TensorBoard logs directory  # Edit this path as needed\n",
    "json_out_config = \"_staged_learning.json\"  # write this TOML to JSON\n",
    "\n",
    "[parameters]\n",
    "\n",
    "use_checkpoint = true  # if true read diversity filter from agent_file\n",
    "purge_memories = false  # if true purge all diversity filter memories after each stage\n",
    "\n",
    "## Reinvent\n",
    "prior_file = \"/Users/devanshjain/REINVENT4/priors/reinvent.prior\"\n",
    "agent_file = \"/Users/devanshjain/laboratoire_d_intelligence_artificielle_en_chimie/transfer/TL_reinvent.model\"\n",
    "\n",
    "batch_size = 128          # network\n",
    "\n",
    "unique_sequences = true  # if true remove all duplicates raw sequences in each step\n",
    "                         # only here for backward compatibility\n",
    "randomize_smiles = true  # if true shuffle atoms in SMILES randomly\n",
    "\n",
    "\n",
    "[learning_strategy]\n",
    "\n",
    "type = \"dap\"      # dap: only one supported\n",
    "sigma = 128       # sigma of the RL reward function\n",
    "rate = 0.0001     # for torch.optim\n",
    "\n",
    "\n",
    "[diversity_filter]  # optional, comment section out or remove if unneeded\n",
    "                    # NOTE: also memorizes all seen SMILES\n",
    "\n",
    "type = \"ScaffoldSimilarity\"      # IdenticalTopologicalScaffold, (SET FOR MAXIMUM NOVELTY - 7xn1 + TACRINE)\n",
    "                                 # ScaffoldSimilarity, PenalizeSameSmiles\n",
    "bucket_size = 50                 # memory size in number of compounds\n",
    "minscore = 0.5                   # only memorize if this threshold is exceeded\n",
    "minsimilarity = 0.2              # minimum similarity for ScaffoldSimilarity\n",
    "penalty_multiplier = 0.7         # penalty factor for PenalizeSameSmiles\n",
    "\n",
    "\n",
    "#smiles_file = \"sampled.smi\"  # \"good\" SMILES for guidance\n",
    "#memory_size = 100  # number of total SMILES held in memory\n",
    "#sample_size = 10  # number of SMILES randomly chosen each epoch\n",
    "\n",
    "\n",
    "### Stage 1\n",
    "### Note that stages must always be a list i.e. double brackets\n",
    "[[stage]]\n",
    "\n",
    "chkpt_file = '/Users/devanshjain/laboratoire_d_intelligence_artificielle_en_chimie/transfer/rl_run.chkpt'  # Edit this checkpoint file path\n",
    "termination = \"simple\"  # termination criterion fot this stage\n",
    "max_score = 0.6  # terminate if this total score is exceeded\n",
    "min_steps = 25  # run for at least this number of steps\n",
    "max_steps = 1000  # terminate entire run when exceeded\n",
    "\n",
    "[stage.scoring]\n",
    "type = \"geometric_mean\"  # aggregation function\n",
    "\n",
    "[[stage.scoring.component]]\n",
    "[[stage.scoring.component.DockStream.endpoint]]\n",
    "name = \"Docking\"\n",
    "weight = 1\n",
    "\n",
    "params.configuration_path = \"{docking_path}\"\n",
    "params.docker_script_path = \"{docker}\"\n",
    "params.docker_python_path =  \"{dockstream_env}/bin/python\"\n",
    "transform.type = \"reverse_sigmoid\"\n",
    "transform.high = -7.5\n",
    "transform.low = -5.0\n",
    "transform.k = 0.25\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9786c932-7395-475d-aec7-86be15f6c8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "staged_path = os.path.join(project_dir, \"staged_config.toml\")\n",
    "\n",
    "# Parse the TOML string\n",
    "staged_dict = toml.loads(staged_toml)\n",
    "\n",
    "# Write the TOML content to a file\n",
    "with open(staged_path, 'w') as f:\n",
    "    toml.dump(staged_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1699bc2e-7cbd-4611-bc21-7951d4c09962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/Users/devanshjain/miniconda3/envs/reinvent4/bin/reinvent\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain_script\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/devanshjain/miniconda3/envs/reinvent4/lib/python3.13/site-packages/reinvent/Reinvent.py\"\u001b[0m, line \u001b[35m195\u001b[0m, in \u001b[35mmain_script\u001b[0m\n",
      "    \u001b[31mmain\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "    \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/devanshjain/miniconda3/envs/reinvent4/lib/python3.13/site-packages/reinvent/Reinvent.py\"\u001b[0m, line \u001b[35m164\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    \u001b[31mrunner\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "    \u001b[31m~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31minput_config=extract_sections(input_config),\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    ...<3 lines>...\n",
      "        \u001b[1;31mwrite_config=write_config,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/Users/devanshjain/miniconda3/envs/reinvent4/lib/python3.13/site-packages/reinvent/runmodes/RL/run_staged_learning.py\"\u001b[0m, line \u001b[35m364\u001b[0m, in \u001b[35mrun_staged_learning\u001b[0m\n",
      "    terminate = optimize(package.terminator)\n",
      "  File \u001b[35m\"/Users/devanshjain/miniconda3/envs/reinvent4/lib/python3.13/site-packages/reinvent/runmodes/RL/learning.py\"\u001b[0m, line \u001b[35m135\u001b[0m, in \u001b[35moptimize\u001b[0m\n",
      "    results = self.score()\n",
      "  File \u001b[35m\"/Users/devanshjain/miniconda3/envs/reinvent4/lib/python3.13/site-packages/reinvent/runmodes/RL/learning.py\"\u001b[0m, line \u001b[35m209\u001b[0m, in \u001b[35mscore\u001b[0m\n",
      "    results = self.scoring_function(\n",
      "        self.sampled.smilies, self.invalid_mask, self.duplicate_mask\n",
      "    )\n",
      "  File \u001b[35m\"/Users/devanshjain/miniconda3/envs/reinvent4/lib/python3.13/site-packages/reinvent/scoring/scorer.py\"\u001b[0m, line \u001b[35m143\u001b[0m, in \u001b[35mcompute_results\u001b[0m\n",
      "    transform_result = compute_transform(\n",
      "        component.component_type,\n",
      "    ...<3 lines>...\n",
      "        valid_mask,\n",
      "    )\n",
      "  File \u001b[35m\"/Users/devanshjain/miniconda3/envs/reinvent4/lib/python3.13/site-packages/reinvent/scoring/compute_scores.py\"\u001b[0m, line \u001b[35m101\u001b[0m, in \u001b[35mcompute_transform\u001b[0m\n",
      "    component_results = compute_component_scores(\n",
      "        smilies, scoring_function, caches[component_type], valid_mask\n",
      "    )\n",
      "  File \u001b[35m\"/Users/devanshjain/miniconda3/envs/reinvent4/lib/python3.13/site-packages/reinvent/scoring/compute_scores.py\"\u001b[0m, line \u001b[35m62\u001b[0m, in \u001b[35mcompute_component_scores\u001b[0m\n",
      "    component_results = scoring_function(smilies_non_cached)\n",
      "  File \u001b[35m\"/Users/devanshjain/miniconda3/envs/reinvent4/lib/python3.13/site-packages/reinvent_plugins/normalize.py\"\u001b[0m, line \u001b[35m22\u001b[0m, in \u001b[35mwrapper\u001b[0m\n",
      "    return func(self, cleaned_smilies)\n",
      "  File \u001b[35m\"/Users/devanshjain/miniconda3/envs/reinvent4/lib/python3.13/site-packages/reinvent_plugins/components/comp_dockstream.py\"\u001b[0m, line \u001b[35m77\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    result = run_command(command)\n",
      "  File \u001b[35m\"/Users/devanshjain/miniconda3/envs/reinvent4/lib/python3.13/site-packages/reinvent_plugins/components/run_program.py\"\u001b[0m, line \u001b[35m26\u001b[0m, in \u001b[35mrun_command\u001b[0m\n",
      "    result = sp.run(command, **args)\n",
      "  File \u001b[35m\"/Users/devanshjain/miniconda3/envs/reinvent4/lib/python3.13/subprocess.py\"\u001b[0m, line \u001b[35m556\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    stdout, stderr = \u001b[31mprocess.communicate\u001b[0m\u001b[1;31m(input, timeout=timeout)\u001b[0m\n",
      "                     \u001b[31m~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/devanshjain/miniconda3/envs/reinvent4/lib/python3.13/subprocess.py\"\u001b[0m, line \u001b[35m1222\u001b[0m, in \u001b[35mcommunicate\u001b[0m\n",
      "    stdout, stderr = \u001b[31mself._communicate\u001b[0m\u001b[1;31m(input, endtime, timeout)\u001b[0m\n",
      "                     \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/devanshjain/miniconda3/envs/reinvent4/lib/python3.13/subprocess.py\"\u001b[0m, line \u001b[35m2128\u001b[0m, in \u001b[35m_communicate\u001b[0m\n",
      "    ready = selector.select(timeout)\n",
      "  File \u001b[35m\"/Users/devanshjain/miniconda3/envs/reinvent4/lib/python3.13/selectors.py\"\u001b[0m, line \u001b[35m398\u001b[0m, in \u001b[35mselect\u001b[0m\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "  File \u001b[35m\"/Users/devanshjain/miniconda3/envs/reinvent4/lib/python3.13/site-packages/reinvent/runmodes/handler.py\"\u001b[0m, line \u001b[35m141\u001b[0m, in \u001b[35m_signal_handler\u001b[0m\n",
      "    raise StageInterruptedUncontrolled(f\"Signal {signum}\")\n",
      "\u001b[1;35mreinvent.runmodes.handler.StageInterruptedUncontrolled\u001b[0m: \u001b[35mSignal 2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!reinvent -l {log_file_reinvent} {staged_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a86373-a631-422f-a243-5914bf07e265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
